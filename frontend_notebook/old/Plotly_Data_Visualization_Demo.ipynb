{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62b7257d",
   "metadata": {},
   "source": [
    "# Data Visualization with Plotly Demo\n",
    "\n",
    "## Introduction to Jupyter Notebook\n",
    "Jupyter Notebooks are a staple in any data scientist's toolkit. It is a free, open source, interactive data science environment that can function as both an IDE and a visualisation tool. A Jupyter Notebook is a single document where you can run code, display the output and add equations and explainations. Each notebook is a `.ipynb` file, which is a text file that describes the content of the notebook in JSON format.\n",
    "\n",
    "Each Jupter Notebook contains a kernal that can be thought of as a \"computational engine\" that executes the code within the notebook. Notebooks are made up of a number of cells. For example, this piece of text you are reading resides in the first cell of this notebook. They can be markdown cells that display text in-place or code cells. When a code cell is run, the output is displayed below the cell. The order in which cells are run matters! Cells containing functions or variables have to be run before those same functions or variables can be called from a subsequent cell. \n",
    "\n",
    "How to use a Jupyter Notebook:\n",
    "- To run a cell, either click the arrow to the left of the cell or press `ctrl + Enter` after selecting the cell. When a cell is run, a number will appear in square brackets (e.g. [1]) telling you the order in which each cell is run.\n",
    "- To interrupt a cell while it is running, press the button with the black square in the toolbar at the top\n",
    "- To restart the kernal, right-click `kernel` and choose from the list of restart options available\n",
    "\n",
    "\n",
    "## Introduction to Plotly\n",
    "\n",
    "Pandas is an open source library providing data structure and data analysis tools for the Python language. Plotly is another open source that allows you to put together high quality graphs to faciliate the visualisation of the data. Plotly Dash (written on top of Plotly.js and React.js) allows one to quickly build data apps that are rendered in the browser. \n",
    "\n",
    "This notebook contains examples of how each of these libraries can be leveraged to analyse and visualise data. For more information, please check out the official documentation listed below.\n",
    "\n",
    "#### Further Documentation\n",
    "https://pandas.pydata.org/docs/ \\\n",
    "https://plot.ly/python/ \\\n",
    "https://dash.plotly.com/introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e9b749",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "You can install the libraries using pip or conda. \n",
    "\n",
    "**N.B.** you may have to restart the kernel after installing these packages for your first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ccdd82",
   "metadata": {},
   "outputs": [
    {
     ]
    }
   ],
   "source": [
    "#!/bin/env python\n",
    "\n",
    "# install packages\n",
    "!pip3 install --user pandas\n",
    "!pip3 install --user numpy\n",
    "!pip3 install --user matplotlib\n",
    "!pip3 install --user plotly\n",
    "!pip3 install --user jupyter-dash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575d5f13",
   "metadata": {},
   "source": [
    "Having installed the libraries, you can import them as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16a4f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "\n",
    "#import plotly\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly import express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from jupyter_dash import JupyterDash\n",
    "from dash import dcc\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output, State\n",
    "\n",
    "# Set display row/column to show all data\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "388e38c6-63e0-4c31-a8ff-f35c5a9b4a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --user plotly\n",
    "#import plotly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8adbeb",
   "metadata": {},
   "source": [
    "## Access Data From Endpoint\n",
    "\n",
    "#### Further Documentation\n",
    "https://docs.python-requests.org/en/master/\n",
    "\n",
    "**N.B.** the url used in this example is from the demo project we have set up. Please replace it with your own url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3c61d47",
   "metadata": {},
   "outputs": [
    {
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define endpoint url\n",
    "url = \"https://dbgee-mar22-12.ew.r.appspot.com/api/text\"\n",
    "\n",
    "# use requests library to send HTTP requests\n",
    "# in this example, GET sentiment analysis data\n",
    "data = json.loads(requests.get(url).text)\n",
    "\n",
    "# examine data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e94864",
   "metadata": {},
   "source": [
    "## Data Visualisation\n",
    "\n",
    "Plotly is a commonly-used data visualisation library. The following examples will show you how to create different graphs from the sample data.\n",
    "\n",
    "We can first read the sample data into a dataframe. The sample data is taken from the UK Met Office and shows the maximum and minimum temperature, the rainfall and the number of hours of sunlight for each month in 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7865a104-acdc-46ff-b25f-07ff1d093028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language_v1\n",
    "\n",
    "def sample_analyze_entities(text_content):\n",
    "    \"\"\"\n",
    "    Analyzing Entities in a String\n",
    "\n",
    "    Args:\n",
    "      text_content The text content to analyze\n",
    "    \"\"\"\n",
    "\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "\n",
    "    # text_content = 'California is a state.'\n",
    "\n",
    "    # Available types: PLAIN_TEXT, HTML\n",
    "    type_ = language_v1.Document.Type.PLAIN_TEXT\n",
    "\n",
    "    # Optional. If not specified, the language is automatically detected.\n",
    "    # For list of supported languages:\n",
    "    # https://cloud.google.com/natural-language/docs/languages\n",
    "    language = \"en\"\n",
    "    document = {\"content\": text_content, \"type_\": type_, \"language\": language}\n",
    "\n",
    "    # Available values: NONE, UTF8, UTF16, UTF32\n",
    "    encoding_type = language_v1.EncodingType.UTF8\n",
    "\n",
    "    response = client.analyze_entities(request = {'document': document, 'encoding_type': encoding_type})\n",
    "    \n",
    "    # TODO add in the filter file here\n",
    "    filterEsgWords = open('articles/ESG_Keywords.txt', \"r\").readlines()\n",
    "    words = [w.lower().strip() for w in filterEsgWords]\n",
    "    # Loop through entitites returned from the API\n",
    "    for entity in response.entities:\n",
    "        \n",
    "        print(u\"Representative name for the entity: {}\".format(entity.name))\n",
    "\n",
    "        if (entity.name.lower() in words):\n",
    "            print(\"We got a match \" + entity.name.lower())\n",
    "        \n",
    "        # Get entity type, e.g. PERSON, LOCATION, ADDRESS, NUMBER, et al\n",
    "        print(u\"Entity type: {}\".format(language_v1.Entity.Type(entity.type_).name))\n",
    "\n",
    "        # Get the salience score associated with the entity in the [0, 1.0] range\n",
    "        print(u\"Salience score: {}\".format(entity.salience))\n",
    "\n",
    "        # Loop over the metadata associated with entity. For many known entities,\n",
    "        # the metadata is a Wikipedia URL (wikipedia_url) and Knowledge Graph MID (mid).\n",
    "        # Some entity types may have additional metadata, e.g. ADDRESS entities\n",
    "        # may have metadata for the address street_name, postal_code, et al.\n",
    "        for metadata_name, metadata_value in entity.metadata.items():\n",
    "            print(u\"{}: {}\".format(metadata_name, metadata_value))\n",
    "\n",
    "        # Loop over the mentions of this entity in the input document.\n",
    "        # The API currently supports proper noun mentions.\n",
    "        for mention in entity.mentions:\n",
    "            print(u\"Mention text: {}\".format(mention.text.content))\n",
    "\n",
    "            # Get the mention type, e.g. PROPER for proper noun\n",
    "            print(\n",
    "                u\"Mention type: {}\".format(language_v1.EntityMention.Type(mention.type_).name)\n",
    "            )\n",
    "\n",
    "    # Get the language of the text, which will be the same as\n",
    "    # the language specified in the request or, if not specified,\n",
    "    # the automatically-detected language.\n",
    "    print(u\"Language of the text: {}\".format(response.language))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ed1b6e64-def7-44d3-9afe-c63c3752dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language_v1\n",
    "\n",
    "def sample_analyze_entity_sentiment(text_content):\n",
    "    \"\"\"\n",
    "    Analyzing Entity Sentiment in a String\n",
    "\n",
    "    Args:\n",
    "      text_content The text content to analyze\n",
    "    \"\"\"\n",
    "\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "\n",
    "    # text_content = 'Grapes are good. Bananas are bad.'\n",
    "\n",
    "    # Available types: PLAIN_TEXT, HTML\n",
    "    type_ = language_v1.types.Document.Type.PLAIN_TEXT\n",
    "\n",
    "    # Optional. If not specified, the language is automatically detected.\n",
    "    # For list of supported languages:\n",
    "    # https://cloud.google.com/natural-language/docs/languages\n",
    "    language = \"en\"\n",
    "    document = {\"content\": text_content, \"type_\": type_, \"language\": language}\n",
    "\n",
    "    # Available values: NONE, UTF8, UTF16, UTF32\n",
    "    encoding_type = language_v1.EncodingType.UTF8\n",
    "\n",
    "    # TODO add in the filter file here\n",
    "    filterEsgWords = open('articles/ESG_Keywords.txt', \"r\").readlines()\n",
    "    words = [w.lower().strip() for w in filterEsgWords]\n",
    "    \n",
    "    response = client.analyze_entity_sentiment(request = {'document': document, 'encoding_type': encoding_type})\n",
    "    # Loop through entitites returned from the API\n",
    "    for entity in response.entities:\n",
    "        print(u\"Representative name for the entity: {}\".format(entity.name))\n",
    "        \n",
    "        if (entity.name.lower() in words):\n",
    "            print(\"We got a match \" + entity.name.lower())\n",
    "            #doing the rest\n",
    "            # Get entity type, e.g. PERSON, LOCATION, ADDRESS, NUMBER, et al\n",
    "            print(u\"Entity type: {}\".format(language_v1.Entity.Type(entity.type_).name))\n",
    "            # Get the salience score associated with the entity in the [0, 1.0] range\n",
    "            print(u\"Salience score: {}\".format(entity.salience))\n",
    "            # Get the aggregate sentiment expressed for this entity in the provided document.\n",
    "            sentiment = entity.sentiment\n",
    "            print(u\"Entity sentiment score: {}\".format(sentiment.score))\n",
    "            print(u\"Entity sentiment magnitude: {}\".format(sentiment.magnitude))\n",
    "            # Loop over the metadata associated with entity. For many known entities,\n",
    "            # the metadata is a Wikipedia URL (wikipedia_url) and Knowledge Graph MID (mid).\n",
    "            # Some entity types may have additional metadata, e.g. ADDRESS entities\n",
    "            # may have metadata for the address street_name, postal_code, et al.\n",
    "            for metadata_name, metadata_value in entity.metadata.items():\n",
    "                print(u\"{} = {}\".format(metadata_name, metadata_value))\n",
    "\n",
    "            # Loop over the mentions of this entity in the input document.\n",
    "            # The API currently supports proper noun mentions.\n",
    "            for mention in entity.mentions:\n",
    "                print(u\"Mention text: {}\".format(mention.text.content))\n",
    "                # Get the mention type, e.g. PROPER for proper noun\n",
    "                print(\n",
    "                    u\"Mention type: {}\".format(language_v1.EntityMention.Type(mention.type_).name)\n",
    "                )\n",
    "        else:\n",
    "            print(\"No ESG match. Binning \" + entity.name.lower())\n",
    "\n",
    "    # Get the language of the text, which will be the same as\n",
    "    # the language specified in the request or, if not specified,\n",
    "    # the automatically-detected language.\n",
    "    print(u\"Language of the text: {}\".format(response.language))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1336d0db-c380-447a-9015-6197abe2cf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c7515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read sample csv data into dataframe\n",
    "arcText = ''\n",
    "\n",
    "for file in os.listdir('articles'):\n",
    "    if not file.startswith('.'): \n",
    "        print(file)\n",
    "        #got our articles\n",
    "        article = open('articles/'+ file, \"r\")\n",
    "        arcText = article.read()\n",
    "        article.close()\n",
    "        #print(arcText)\n",
    "        #sample_analyze_entities(arcText)\n",
    "        sample_analyze_entity_sentiment(arcText)\n",
    "        #call POST on our endpoint\n",
    "        #result = json.loads(requests.post(url).text)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696380db-1907-4f5b-809c-0344c1abf41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arcText)\n",
    "response = os.exec('curl -X 'POST' \\\n",
    "  'https://dbgee-mar22-12.ew.r.appspot.com/api/text' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/x-www-form-urlencoded' \\\n",
    "  -d 'text=buy%20flowers%20for%20the%20weekend%20cos%20it'\\''s%20mother'\\''s%20day'')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca14ef",
   "metadata": {},
   "source": [
    "To gain more insight into a particular column, you can use the *describe()* method on the dataframe column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ca4e3d79-97d9-4865-a8e6-42c5f0d4ff6a",
   "metadata": {},
   "outputs": [
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58d48ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    12.000000\n",
      "mean     48.333333\n",
      "std      24.630555\n",
      "min       0.400000\n",
      "25%      29.300000\n",
      "50%      58.200000\n",
      "75%      62.050000\n",
      "max      81.200000\n",
      "Name: Rain, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# describe the monthly rainfall\n",
    "rain_data = weather.Rain.describe()\n",
    "print(rain_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b793c",
   "metadata": {},
   "source": [
    "#### 1D Line Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5eee88bf",
   "metadata": {},
   "outputs": [
    {
      ],
      "text/plain": [
       "   Year  Month  Tmax  Tmin  Rain    Sun   Tmed\n",
       "0  2018      1   9.7   3.8  58.0   46.5   6.75\n",
       "1  2018      2   6.7   0.6  29.0   92.0   3.65\n",
       "2  2018      3   9.8   3.0  81.2   70.3   6.40\n",
       "3  2018      4  15.5   7.9  65.2  113.4  11.70\n",
       "4  2018      5  20.8   9.8  58.4  248.3  15.30"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate ave. temp. and create a new column in the dataframe \n",
    "weather['Tmed'] = (weather['Tmax'] + weather['Tmin'])/2\n",
    "\n",
    "# inspect the first 5 rows\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8ff6d32",
   "metadata": {},
   "outputs": [
    {
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot max and min temp. vs month\n",
    "min_temp = go.Scatter(x=weather['Month'], y=weather['Tmin'], name='Min Temp')\n",
    "med_temp = go.Scatter(x=weather['Month'], y=weather['Tmed'], name='Ave Temp')\n",
    "max_temp = go.Scatter(x=weather['Month'], y=weather['Tmax'], name='Max Temp')\n",
    "\n",
    "min_max_temp_fig = go.Figure()\n",
    "\n",
    "min_max_temp_fig.add_trace(min_temp)\n",
    "min_max_temp_fig.add_trace(med_temp)\n",
    "min_max_temp_fig.add_trace(max_temp)\n",
    "\n",
    "# edit the layout\n",
    "min_max_temp_fig.update_layout(title=\"Temperature Distribution\",\n",
    "                               xaxis_title='Month',\n",
    "                               yaxis_title='Temperature (Celsius)')\n",
    "\n",
    "min_max_temp_fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4b211",
   "metadata": {},
   "source": [
    "#### Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d33ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lot rainfall vs month\n",
    "\n",
    "rainfall_fig = px.bar(weather, x='Month', y='Rain')\n",
    "rainfall_fig.update_layout(title=\"Rainfall Distribution\",\n",
    "                           xaxis_title=\"Month\",\n",
    "                           yaxis_title='Rain')\n",
    "rainfall_fig.show()\n",
    "\n",
    "# The following line of code achieves the same thing using Matplotlib\n",
    "\n",
    "# weather.plot.bar(y='Rain', x='Month')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae08d9a",
   "metadata": {},
   "source": [
    "#### Histogram\n",
    "\n",
    "Histograms are useful for when you want to visualise the frequency distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293dc65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfall_hist = px.histogram(weather, x='Rain', nbins=10) # you can specify the number of bins\n",
    "rainfall_hist.update_layout(title=\"Frequency of Rainfall Amount\",\n",
    "                            bargap=0.1) # you can specify a gap between bars\n",
    "rainfall_hist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f381e9",
   "metadata": {},
   "source": [
    "#### Multiple Charts\n",
    "\n",
    "You can also create separate charts for each column of data. The following example shows separate line graphs of the four columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536a2178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple line charts \n",
    "rain = go.Scatter(x=weather['Month'], y=weather['Rain'], name=\"Rain\")\n",
    "sun = go.Scatter(x=weather['Month'], y=weather['Sun'], name=\"Sun\")\n",
    "\n",
    "subplots_fig = make_subplots(rows=2, cols=2,\n",
    "                             subplot_titles=(\"Min Temp\", \"Max Temp\", \"Rain\", \"Sun\"))\n",
    "\n",
    "# use min_temp and max_temp plots from before\n",
    "subplots_fig.add_trace(min_temp, row=1, col=1)\n",
    "subplots_fig.add_trace(max_temp, row=2, col=1)\n",
    "subplots_fig.add_trace(rain, row=1, col=2)\n",
    "subplots_fig.add_trace(sun, row=2, col=2)\n",
    "\n",
    "subplots_fig.update_layout(height=600, width=800, title_text=\"Subplots Demo\")\n",
    "\n",
    "subplots_fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddb9da5",
   "metadata": {},
   "source": [
    "## Introducing Jupyter Dash\n",
    "\n",
    "Dash is Plotly's open source Python framework for building full stack analytic web applications using pure Python. The JupyterDash library makes these features available from the jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f830387",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run ngrok to tunnel Dash app port 8050 to the outside world. \n",
    "### This command runs in the background.\n",
    "get_ipython().system_raw('./ngrok http 8050 &')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63687a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ID of the most recent \n",
    "last_text_id = list(data.keys())[0]\n",
    "\n",
    "app = JupyterDash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"JupyterDash Demo\"),\n",
    "    \n",
    "    \n",
    "    # THESE LINES DISPLAY THE OUTPUT OF NLP API\n",
    "    html.P(\"Most Recent Text ID: {}\".format(last_text_id)),\n",
    "    html.P(\"Text Analysed: {}\".format(data[last_text_id][\"text\"])),\n",
    "    html.P(\"Sentiment: {}\".format(data[last_text_id][\"sentiment\"])),\n",
    "  \n",
    "    # THESE LINES DEMO ONE OF THE DASH CORE COMPONENT(dcc) i.e. dcc.Input\n",
    "    html.H3(\"Change the value in the text box to see callbacks in action!\"),\n",
    "    html.Div([\n",
    "        \"Input: \",\n",
    "        dcc.Input(id='my-input', value='initial value', type='text')\n",
    "    ]),\n",
    "    html.Br(),\n",
    "    html.Div(id='my-output'),\n",
    "    \n",
    "    # THESE LINES DEMO THE INTEGRATION OF PLOTLY GRAPHS WITH DASH\n",
    "    dcc.Graph(figure=subplots_fig),\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output(component_id='my-output', component_property='children'),\n",
    "    Input(component_id='my-input', component_property='value')\n",
    ")\n",
    "def update_output_div(input_value):\n",
    "    return 'Output: {}'.format(input_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928b561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.run_server(mode=\"external\", port=8050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b36aff",
   "metadata": {},
   "source": [
    "#### In case the below cell has errors, please rerun it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc38fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the public URL where you can access the Dash app. Copy this URL.\n",
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f45f323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
